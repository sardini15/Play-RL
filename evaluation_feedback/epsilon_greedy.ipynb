{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epsilon_greedy\n",
    "\n",
    "『強化学習』第1部第2章「評価フィードバック」第2節「行動価値手法」より、多腕バンディット問題における $\\epsilon$-greedy手法とgreedy手法の性能比較を実装する。"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 多腕バンディット問題について\n",
    "\n",
    "多腕バンディット問題のもっとも単純な例：  \n",
    "\n",
    "n種類の異なる行動選択肢があり、エージェントはその中から1つを選ぶという動作を繰り返す。  \n",
    "各選択の後に毎回、ある数値が選択され、その値が報酬として渡される。この数値は、選んだ行動に依存する定常確率分布に従う。  \n",
    "目的は、ある期間の行動選択で、合計報酬の期待値を最大化することである。"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## $\\epsilon$-greedy手法とgreedy手法について\n",
    "\n",
    "### 知識利用と探査\n",
    "\n",
    "多腕バンディット問題では、それぞれの行動について、その行動が選ばれた場合の報酬の期待値が定まっている。この値を、行動の価値と呼ぶ。  \n",
    "もしそれぞれの行動の価値があらかじめ分かっているなら、常にもっとも高い価値を持つ行動を選択してゆけば合計報酬の期待値を最大化できる。  \n",
    "\n",
    "ここでは行動の価値が確実には知られていないと仮定する。  \n",
    "その場合でも、行動価値の推定値をつねに持っていれば、どの時点でも、価値の推定値を最大にするような行動が存在する。このような行動を、greedy な行動と呼ぶ。  \n",
    "\n",
    "つねにgreedyな行動を選択することが、合計報酬の期待値を最大化するとは限らない。greedyでない行動を選択することで、その価値の推定値を更新できる可能性があるためである。  \n",
    "多腕バンディット問題では、greedyな行動(知識利用)とgreedyでない行動(探査)との競合が問題となる。  \n",
    "\n",
    "一般に、知識利用をつねに行い続けるよりも、何らかのバランシング手法を用いて知識利用と探査の両方を行う方が、性能が遥かに高いことが知られている。  \n",
    "ここでは、もっとも単純なバランシング手法として、$\\epsilon$-greedy手法を考える。\n",
    "\n",
    "### $\\epsilon$-greedy手法\n",
    "\n",
    "つねに知識利用を行い続ける方法をgreedy手法と呼ぶ。  \n",
    "対して、$\\epsilon$-greedy手法とは、ほとんどの場合知識利用を行うが、たまに小さい確率$\\epsilon$で行動価値推定量とは無関係にランダムな行動を選ぶ方法である。"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aaa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-972a1a11f199>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maaa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'aaa' is not defined"
     ]
    }
   ],
   "source": [
    "## 実装の詳細\n",
    "\n",
    "ランダムに生成した2000個の10本腕バンディットタスクを扱う。  \n",
    "行動$a$に対する報酬は、平均$Q^*(a)$, 分散1の正規分布から選ぶ。  \n",
    "ここで$Q^*(a)$は、標準正規分布からランダムに2000回選び出した値であり、これにより各タスクにおける報酬の分布が決定される。  \n",
    "全てのタスクについて報酬の平均を取り、1000回のプレイでの経験による改善を確認する。  \n",
    "これをgreedy手法と$\\epsilon$-greedy手法($\\epsilon = 0.01, 0.1$)について行い、比較する。"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 行動価値の推定方法\n",
    "\n",
    "行動$a$の真の価値を$Q^*(a)$, $t$番目のプレイにおけるその推定量を$Q_{t}(a)$と表記する。  \n",
    "各行動の真の価値は、その行動によって受け取る報酬の期待値である。これを推定する方法の一つとして、標本平均手法がある。  \n",
    "$$\n",
    "Q_{t}(a) = \\frac{r_{1}+r_{2}+\\cdots+r_{k_{a}}}{k_{a}}\n",
    "$$  \n",
    "ここで、$k_{a}$は$t$番目のプレイにおいて行動$a$がそれまで選択された回数、$r_{i}$はそれぞれの回で受け取る報酬。  \n",
    "$k_{a}=0$の場合には、$Q_{0}(a)=0$のようなデフォルト値に設定する。  \n",
    "$k_{a} \\to \\infty$ の極限において、大数の法則から$Q_{t}(a)$は$Q^*(a)$に収束する。  \n",
    "\n",
    "標本平均手法は数ある行動価値推定方法の一手法であり、必ずしも最良の方法ではないが、今回の実装ではこれを利用する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}